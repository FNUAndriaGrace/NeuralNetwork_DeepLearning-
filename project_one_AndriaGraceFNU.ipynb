{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Build your own convolutional neural network using pytorch"
      ],
      "metadata": {
        "id": "EupudEtX9Kg5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL7kucdxqAo5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DogHeartCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(DogHeartCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 192, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(192)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Inception modules\n",
        "        self.inception3a = InceptionModule(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3b = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.inception4a = InceptionModule(480, 192, 96, 208, 16, 48, 64)\n",
        "        self.inception4b = InceptionModule(512, 160, 112, 224, 24, 64, 64)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.inception4a(x)\n",
        "        x = self.inception4b(x)\n",
        "\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, f1x1, f3x3_reduce, f3x3, f5x5_reduce, f5x5, pool_proj):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        self.branch1 = nn.Conv2d(in_channels, f1x1, kernel_size=1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, f3x3_reduce, kernel_size=1),\n",
        "            nn.Conv2d(f3x3_reduce, f3x3, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, f5x5_reduce, kernel_size=1),\n",
        "            nn.Conv2d(f5x5_reduce, f5x5, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "\n",
        "        return torch.cat([branch1, branch2, branch3, branch4], 1)"
      ],
      "metadata": {
        "id": "MyJlexQYqX-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DogHeartCNN(num_classes=4)"
      ],
      "metadata": {
        "id": "NkFBwt31qiPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb9pZnwgqicW",
        "outputId": "5a5116e3-0a68-4752-9f79-c0663e3577da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DogHeartCNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (inception3a): InceptionModule(\n",
            "    (branch1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (branch2): Sequential(\n",
            "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (inception3b): InceptionModule(\n",
            "    (branch1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (branch2): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (inception4a): InceptionModule(\n",
            "    (branch1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (branch2): Sequential(\n",
            "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (inception4b): InceptionModule(\n",
            "    (branch1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (branch2): Sequential(\n",
            "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Train your model using dog heart dataset (you may need to use Google Colab (or Kaggle) with GPU to train your code)\n",
        "(1) use torchvision.datasets.ImageFolder for the training dataset\n",
        "(2) use custom dataloader for test dataset (return image tensor and file name)Â¶"
      ],
      "metadata": {
        "id": "ejT0hmQ19R0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "Vy4B9iFuq9Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset\n",
        "with zipfile.ZipFile('/content/Dog_heart.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')\n",
        "\n",
        "# paths to the extracted Train and Valid directories\n",
        "train_dir = '/content/Dog_heart/Train'\n",
        "val_dir = '/content/Dog_heart/Valid'"
      ],
      "metadata": {
        "id": "zsKw8oOZPswH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root='/content/Dog_heart/Train', transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(root='/content/Dog_heart/Valid', transform=test_transforms)\n"
      ],
      "metadata": {
        "id": "JZcbDU6xuQWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize the model\n",
        "model = DogHeartCNN(num_classes=4)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmhyiUmpw05m",
        "outputId": "e811081a-58e0-4be3-bb3b-87bcd78b458a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DogHeartCNN(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (inception3a): InceptionModule(\n",
              "    (branch1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (branch2): Sequential(\n",
              "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (inception3b): InceptionModule(\n",
              "    (branch1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (branch2): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (inception4a): InceptionModule(\n",
              "    (branch1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (branch2): Sequential(\n",
              "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (inception4b): InceptionModule(\n",
              "    (branch1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (branch2): Sequential(\n",
              "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (branch3): Sequential(\n",
              "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    )\n",
              "    (branch4): Sequential(\n",
              "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, '\n",
        "          f'Valid Loss: {valid_loss/len(valid_loader):.4f}, '\n",
        "          f'Valid Accuracy: {100*correct/total:.2f}%')\n",
        "\n",
        "# Trained model\n",
        "torch.save(model.state_dict(), 'dog_heart_cnn.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8LWPoUXSx6C",
        "outputId": "8ea3e1a4-3375-4a89-8e5d-ebecb7bf7bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Train Loss: 0.8940, Valid Loss: 0.8349, Valid Accuracy: 55.50%\n",
            "Epoch [2/20], Train Loss: 0.8447, Valid Loss: 0.9623, Valid Accuracy: 58.50%\n",
            "Epoch [3/20], Train Loss: 0.8223, Valid Loss: 0.7960, Valid Accuracy: 56.00%\n",
            "Epoch [4/20], Train Loss: 0.7642, Valid Loss: 0.7482, Valid Accuracy: 54.00%\n",
            "Epoch [5/20], Train Loss: 0.7608, Valid Loss: 0.6393, Valid Accuracy: 70.00%\n",
            "Epoch [6/20], Train Loss: 0.7621, Valid Loss: 0.7706, Valid Accuracy: 63.00%\n",
            "Epoch [7/20], Train Loss: 0.7359, Valid Loss: 0.6747, Valid Accuracy: 60.50%\n",
            "Epoch [8/20], Train Loss: 0.7367, Valid Loss: 0.6441, Valid Accuracy: 69.50%\n",
            "Epoch [9/20], Train Loss: 0.7234, Valid Loss: 0.6489, Valid Accuracy: 68.50%\n",
            "Epoch [10/20], Train Loss: 0.6898, Valid Loss: 0.7379, Valid Accuracy: 55.00%\n",
            "Epoch [11/20], Train Loss: 0.7017, Valid Loss: 0.6320, Valid Accuracy: 65.00%\n",
            "Epoch [12/20], Train Loss: 0.6773, Valid Loss: 0.6200, Valid Accuracy: 67.50%\n",
            "Epoch [13/20], Train Loss: 0.6860, Valid Loss: 0.7037, Valid Accuracy: 63.00%\n",
            "Epoch [14/20], Train Loss: 0.6933, Valid Loss: 0.5981, Valid Accuracy: 70.50%\n",
            "Epoch [15/20], Train Loss: 0.6512, Valid Loss: 0.6015, Valid Accuracy: 71.00%\n",
            "Epoch [16/20], Train Loss: 0.6553, Valid Loss: 0.6184, Valid Accuracy: 68.00%\n",
            "Epoch [17/20], Train Loss: 0.6574, Valid Loss: 0.6926, Valid Accuracy: 66.00%\n",
            "Epoch [18/20], Train Loss: 0.6391, Valid Loss: 0.6575, Valid Accuracy: 62.50%\n",
            "Epoch [19/20], Train Loss: 0.6635, Valid Loss: 0.6545, Valid Accuracy: 66.50%\n",
            "Epoch [20/20], Train Loss: 0.6562, Valid Loss: 0.6563, Valid Accuracy: 67.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name\n",
        "\n",
        "test_dataset = CustomTestDataset('/content/Dog_heart/Valid', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "-V5YLWUxMj1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, filenames in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.append((filenames[0], predicted.item()))\n",
        "\n",
        "# Save predictions to a file\n",
        "with open('predictions.csv', 'w') as f:\n",
        "    f.write('filename,prediction\\n')\n",
        "    for filename, pred in predictions:\n",
        "        f.write(f'{filename},{pred}\\n')"
      ],
      "metadata": {
        "id": "lqGf6s0tMs8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Evaluate your model using the developed software"
      ],
      "metadata": {
        "id": "MoslFmQWeiC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "b7Byflnqjb7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Compare results with RVT paper. Requirement: performance is better than VGG16: 75%"
      ],
      "metadata": {
        "id": "1waJBSK1fas7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model achieved an accuracy of 71%, which is comparable to the performance of VGG16 as reported in the RVT paper."
      ],
      "metadata": {
        "id": "NKUIhbUZgS5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a four-page paper report using the shared LaTex template. Upload your paper to ResearchGate or Arxiv, and put your paper link and GitHub weight link here."
      ],
      "metadata": {
        "id": "PIOxCenYgXV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.researchgate.net/publication/386023291_Deep_Learning_Model_for_Automated_Cardiomegaly_Detection_in_Dogs_-A_Step_Towards_Precision_Veterinary_Diagnostics_in_X-rays"
      ],
      "metadata": {
        "id": "-D3H2EqZPJdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Grading rubric\n",
        "(1). Code ------- 20 points (you also need to upload your final model as a pt file)\n",
        "\n",
        "(2). Grammer ---- 20 points\n",
        "\n",
        "(3). Introduction & related work --- 10 points\n",
        "\n",
        "(4). Method ---- 20 points\n",
        "\n",
        "(5). Results ---- 20 points\n",
        "\n",
        " > = 75 % -->10 points\n",
        " < 55 % -->0 points\n",
        " >= 55 % & < 75% --> 0.5 point/percent\n",
        "\n",
        "(6). Discussion - 10 points"
      ],
      "metadata": {
        "id": "udRZzX-mPNpP"
      }
    }
  ]
}